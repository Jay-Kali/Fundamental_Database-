{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fa8afc0-f5aa-4588-8bb2-bc227109ac72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/All E Tech.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Allied Blenders.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Anand Rathi Wea.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Anant Raj.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Apar Inds.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Blue Jet Health.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Caplin Point Lab.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/CEINSYS Tech.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Data Pattern.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/E2E Networks.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Fidel Softech.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/FSN E-Commerce.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Genus Power.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Goldiam Intl.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Indrapr.Medical.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/KPI Green Energy.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Marksans Pharma.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Mastek.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Natco Pharma.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Nuvama Wealth.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Pearl Global Ind.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/PNGS Gargi FJ.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Pokarna.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/RBM Infracon.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Reliance Industr.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Shilchar Tech.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Tilaknagar Inds.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Transrail Light.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Volt.Transform.xlsx\n",
      "ğŸ“„ Processing file: /mnt/c/Bhavcopy/NSE_FundaMenal_Source/Zaggle Prepaid.xlsx\n",
      "âœ… All files processed and appended.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from openpyxl import load_workbook\n",
    "import hashlib\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# === Path Setup ===\n",
    "source_folder = '/mnt/c/Bhavcopy/NSE_FundaMenal_Source/'\n",
    "final_output_path = '/mnt/c/Bhavcopy/NSE_Funda/Temp_Yearly_Result.xlsx'\n",
    "sheet_name = 'Data Sheet'\n",
    "\n",
    "# Expected fiscal years (you can expand as needed)\n",
    "expected_years = ['Mar-20', 'Mar-21', 'Mar-22', 'Mar-23', 'Mar-24', 'Mar-25']\n",
    "\n",
    "# === Step 1: Loop through all Excel files in the source folder ===\n",
    "excel_files = glob.glob(os.path.join(source_folder, '*.xlsx'))\n",
    "\n",
    "for source_path in excel_files:\n",
    "    print(f\"ğŸ“„ Processing file: {source_path}\")\n",
    "\n",
    "    try:\n",
    "        # === Step 2: Read metadata from B1, B2, B3 ===\n",
    "        meta_values = pd.read_excel(\n",
    "            source_path,\n",
    "            sheet_name=sheet_name,\n",
    "            usecols='B',\n",
    "            nrows=3,\n",
    "            header=None\n",
    "        ).squeeze()\n",
    "\n",
    "        company_name = meta_values.iloc[0]\n",
    "        latest_version = meta_values.iloc[1]\n",
    "        current_version = meta_values.iloc[2]\n",
    "\n",
    "        # === Step 3: Read and transpose P&L block (rows 16â€“31) ===\n",
    "        df_pl = pd.read_excel(source_path, sheet_name=sheet_name, skiprows=15, nrows=16)\n",
    "        df_pl.set_index(df_pl.columns[0], inplace=True)\n",
    "        df_pl = df_pl.T\n",
    "        df_pl.index = pd.to_datetime(df_pl.index, format='%b-%y', errors='coerce')  # âœ… Avoid warning\n",
    "        df_pl.index.name = 'Report_Date'\n",
    "\n",
    "        # === Step 4: Helper function to read and transpose other blocks ===\n",
    "        def read_transposed(skiprows, nrows):\n",
    "            df = pd.read_excel(source_path, sheet_name=sheet_name, skiprows=skiprows, nrows=nrows)\n",
    "            df.set_index(df.columns[0], inplace=True)\n",
    "            df = df.T\n",
    "            df.index = df_pl.index  # Align with P&L dates\n",
    "            return df\n",
    "\n",
    "        # === Step 5: Read remaining blocks ===\n",
    "        df2_t = read_transposed(55, 17)\n",
    "        df3_t = read_transposed(80, 5)\n",
    "\n",
    "        # === Step 6: PRICE and DERIVED rows using header for dates ===\n",
    "        header_row = pd.read_excel(source_path, sheet_name=sheet_name, skiprows=80, nrows=1, header=None)\n",
    "        date_values = pd.to_datetime(header_row.iloc[0, 1:].values, format='%b-%y', errors='coerce')\n",
    "\n",
    "        df4 = pd.read_excel(source_path, sheet_name=sheet_name, skiprows=89, nrows=1, header=None)\n",
    "        df4_t = pd.DataFrame([df4.iloc[0, 1:].values], columns=date_values).T\n",
    "        df4_t.columns = [df4.iloc[0, 0]]\n",
    "        df4_t.index = df_pl.index\n",
    "\n",
    "        df5 = pd.read_excel(source_path, sheet_name=sheet_name, skiprows=92, nrows=1, header=None)\n",
    "        df5_t = pd.DataFrame([df5.iloc[0, 1:].values], columns=date_values).T\n",
    "        df5_t.columns = [df5.iloc[0, 0]]\n",
    "        df5_t.index = df_pl.index\n",
    "\n",
    "        # === Step 7: Merge all blocks ===\n",
    "        df_combined = pd.concat([df_pl, df2_t, df3_t, df4_t, df5_t], axis=1)\n",
    "        df_combined.reset_index(inplace=True)\n",
    "        df_combined['Report_Date'] = pd.to_datetime(df_combined['Report_Date']).dt.date\n",
    "        df_combined.insert(1, 'Company_Name', company_name)\n",
    "        df_combined.insert(2, 'Latest_Version', latest_version)\n",
    "        df_combined.insert(3, 'Current_Version', current_version)\n",
    "\n",
    "        # === Step 8: Clean column names ===\n",
    "        df_combined.columns = [\n",
    "            str(col).replace(' ', '_').replace('.', '').replace('__', '_').strip()\n",
    "            for col in df_combined.columns\n",
    "        ]\n",
    "\n",
    "        # === Step 9: Remove rows where Report_Date and Sales are both empty ===\n",
    "        if 'Sales' in df_combined.columns:\n",
    "            df_combined = df_combined[~(df_combined['Report_Date'].isna() & df_combined['Sales'].isna())]\n",
    "        else:\n",
    "            print(f\"âš ï¸ 'Sales' column missing in {os.path.basename(source_path)} â€” skipping file.\")\n",
    "            continue\n",
    "\n",
    "        # === Step 10: Add MD5 hash column ===\n",
    "        def row_to_md5(row):\n",
    "            row_string = '|'.join(str(val) for val in row.values)\n",
    "            return hashlib.md5(row_string.encode('utf-8')).hexdigest()\n",
    "\n",
    "        df_combined['Row_Hash_MD5'] = df_combined.apply(row_to_md5, axis=1)\n",
    "\n",
    "        # === Step 11: Append to final Excel file ===\n",
    "        if not os.path.exists(final_output_path):\n",
    "            df_combined.to_excel(final_output_path, index=False)\n",
    "        else:\n",
    "            with pd.ExcelWriter(final_output_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
    "                existing_wb = load_workbook(final_output_path)\n",
    "                ws = existing_wb.active\n",
    "                start_row = ws.max_row\n",
    "                df_combined.to_excel(writer, index=False, header=False, startrow=start_row)\n",
    "\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            header_row = pd.read_excel(source_path, sheet_name=sheet_name, skiprows=15, nrows=1, header=None)\n",
    "            actual_years = header_row.iloc[0, 1:].astype(str).tolist()\n",
    "            missing_years = [y for y in expected_years if y not in actual_years]\n",
    "            if missing_years:\n",
    "                print(f\"âš ï¸ Skipped file {os.path.basename(source_path)} â€” Missing fiscal years: {', '.join(missing_years)}\")\n",
    "            else:\n",
    "                print(f\"âŒ Failed to process file {os.path.basename(source_path)}: {e}\")\n",
    "        except Exception:\n",
    "            print(f\"âŒ Failed to process file {os.path.basename(source_path)}: {e}\")\n",
    "\n",
    "print(\"âœ… All files processed and appended.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5829e1-9347-4c9a-8de5-659f2847ae2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
