{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a8bf72c-11d9-4859-8cce-5c3d6f5f61f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Processing: All E Tech.xlsx\n",
      "✅ Created file and added 10 rows.\n",
      "\n",
      "📄 Processing: Allied Blenders.xlsx\n",
      "✅ Appended 10 rows at row 12\n",
      "\n",
      "📄 Processing: Anand Rathi Wea.xlsx\n",
      "✅ Appended 10 rows at row 22\n",
      "\n",
      "📄 Processing: Anant Raj.xlsx\n",
      "✅ Appended 10 rows at row 32\n",
      "\n",
      "📄 Processing: Apar Inds.xlsx\n",
      "✅ Appended 10 rows at row 42\n",
      "\n",
      "📄 Processing: Blue Jet Health.xlsx\n",
      "✅ Appended 10 rows at row 52\n",
      "\n",
      "📄 Processing: Caplin Point Lab.xlsx\n",
      "✅ Appended 10 rows at row 62\n",
      "\n",
      "📄 Processing: CEINSYS Tech.xlsx\n",
      "✅ Appended 10 rows at row 72\n",
      "\n",
      "📄 Processing: Data Pattern.xlsx\n",
      "✅ Appended 10 rows at row 82\n",
      "\n",
      "📄 Processing: E2E Networks.xlsx\n",
      "✅ Appended 10 rows at row 92\n",
      "\n",
      "📄 Processing: Fidel Softech.xlsx\n",
      "✅ Appended 10 rows at row 102\n",
      "\n",
      "📄 Processing: FSN E-Commerce.xlsx\n",
      "✅ Appended 10 rows at row 112\n",
      "\n",
      "📄 Processing: Genus Power.xlsx\n",
      "✅ Appended 10 rows at row 122\n",
      "\n",
      "📄 Processing: Goldiam Intl.xlsx\n",
      "✅ Appended 10 rows at row 132\n",
      "\n",
      "📄 Processing: Indrapr.Medical.xlsx\n",
      "✅ Appended 10 rows at row 142\n",
      "\n",
      "📄 Processing: KPI Green Energy.xlsx\n",
      "✅ Appended 10 rows at row 152\n",
      "\n",
      "📄 Processing: Marksans Pharma.xlsx\n",
      "✅ Appended 10 rows at row 162\n",
      "\n",
      "📄 Processing: Mastek.xlsx\n",
      "✅ Appended 10 rows at row 172\n",
      "\n",
      "📄 Processing: Natco Pharma.xlsx\n",
      "✅ Appended 10 rows at row 182\n",
      "\n",
      "📄 Processing: Nuvama Wealth.xlsx\n",
      "✅ Appended 10 rows at row 192\n",
      "\n",
      "📄 Processing: Pearl Global Ind.xlsx\n",
      "✅ Appended 10 rows at row 202\n",
      "\n",
      "📄 Processing: PNGS Gargi FJ.xlsx\n",
      "✅ Appended 10 rows at row 212\n",
      "\n",
      "📄 Processing: Pokarna.xlsx\n",
      "✅ Appended 10 rows at row 222\n",
      "\n",
      "📄 Processing: RBM Infracon.xlsx\n",
      "✅ Appended 10 rows at row 232\n",
      "\n",
      "📄 Processing: Reliance Industr.xlsx\n",
      "✅ Appended 10 rows at row 242\n",
      "\n",
      "📄 Processing: Shilchar Tech.xlsx\n",
      "✅ Appended 10 rows at row 252\n",
      "\n",
      "📄 Processing: Tilaknagar Inds.xlsx\n",
      "✅ Appended 10 rows at row 262\n",
      "\n",
      "📄 Processing: Transrail Light.xlsx\n",
      "✅ Appended 10 rows at row 272\n",
      "\n",
      "📄 Processing: Volt.Transform.xlsx\n",
      "✅ Appended 10 rows at row 282\n",
      "\n",
      "📄 Processing: Zaggle Prepaid.xlsx\n",
      "✅ Appended 10 rows at row 292\n",
      "\n",
      "🧹 Cleanup complete: Removed 4 invalid rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "import os\n",
    "import glob\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# === Path Setup ===\n",
    "source_path = '/mnt/c/Bhavcopy/NSE_FundaMenal_Source/'\n",
    "final_output_path = '/mnt/c/Bhavcopy/NSE_Funda/Temp_Quarter_result.xlsx'\n",
    "sheet_name = 'Data Sheet'\n",
    "\n",
    "# === Collect all .xlsx files in source_path\n",
    "excel_files = glob.glob(os.path.join(source_path, '*.xlsx'))\n",
    "write_header_once = not os.path.exists(final_output_path)\n",
    "\n",
    "for file_path in excel_files:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    print(f\"\\n📄 Processing: {file_name}\")\n",
    "\n",
    "    try:\n",
    "        # Step 1: Read metadata from B1:B3\n",
    "        meta_values = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            usecols='B',\n",
    "            nrows=3,\n",
    "            header=None\n",
    "        ).squeeze()\n",
    "\n",
    "        company_name = meta_values.iloc[0]\n",
    "        latest_version = meta_values.iloc[1]\n",
    "        current_version = meta_values.iloc[2]\n",
    "\n",
    "        # Step 2: Read financials from rows 41–50, A–K\n",
    "        df_raw = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            skiprows=40,\n",
    "            nrows=10,\n",
    "            usecols='A:K',\n",
    "            header=None\n",
    "        )\n",
    "\n",
    "        # Step 3: Set column A as index\n",
    "        df_raw.set_index(df_raw.columns[0], inplace=True)\n",
    "\n",
    "        # Step 4: Transpose\n",
    "        df_qtr = df_raw.T\n",
    "        df_qtr.columns.name = None\n",
    "\n",
    "        # Step 5: Extract actual report dates from row 41, B–K\n",
    "        original_columns = pd.read_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            skiprows=40,\n",
    "            nrows=1,\n",
    "            usecols='B:K',\n",
    "            header=None\n",
    "        ).iloc[0].tolist()\n",
    "\n",
    "        report_dates = pd.to_datetime(original_columns, format='%b-%y', errors='coerce')\n",
    "\n",
    "        # Step 6: Insert clean Report_Date\n",
    "        df_qtr.insert(0, 'Report_Date', report_dates.date)\n",
    "\n",
    "        # Step 7: Drop duplicate column if exists\n",
    "        if 'Report Date' in df_qtr.columns:\n",
    "            df_qtr.drop(columns=['Report Date'], inplace=True)\n",
    "\n",
    "        # Step 8: Add metadata\n",
    "        df_qtr.insert(1, 'Company_Name', company_name)\n",
    "        df_qtr.insert(2, 'Latest_Version', latest_version)\n",
    "        df_qtr.insert(3, 'Current_Version', current_version)\n",
    "\n",
    "        # Step 9: Add MD5 hash\n",
    "        def row_to_md5(row):\n",
    "            row_string = '|'.join(str(val) for val in row.values)\n",
    "            return hashlib.md5(row_string.encode('utf-8')).hexdigest()\n",
    "\n",
    "        df_qtr['Row_Hash_MD5'] = df_qtr.apply(row_to_md5, axis=1)\n",
    "\n",
    "        # Step 10: Append to Excel\n",
    "        if write_header_once:\n",
    "            df_qtr.to_excel(final_output_path, index=False)\n",
    "            write_header_once = False\n",
    "            print(f\"✅ Created file and added {len(df_qtr)} rows.\")\n",
    "        else:\n",
    "            with pd.ExcelWriter(final_output_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
    "                existing_wb = load_workbook(final_output_path)\n",
    "                ws = existing_wb.active\n",
    "                start_row = ws.max_row\n",
    "                df_qtr.to_excel(writer, index=False, header=False, startrow=start_row)\n",
    "                print(f\"✅ Appended {len(df_qtr)} rows at row {start_row + 1}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in {file_name}: {e}\")\n",
    "\n",
    "# === Final Cleanup: Remove rows with empty Report_Date and Sales ===\n",
    "try:\n",
    "    df_final = pd.read_excel(final_output_path)\n",
    "\n",
    "    # Ensure 'Report_Date' is datetime (handles first and second run)\n",
    "    if 'Report_Date' in df_final.columns:\n",
    "        df_final['Report_Date'] = pd.to_datetime(df_final['Report_Date'], errors='coerce')\n",
    "\n",
    "    if 'Sales' in df_final.columns:\n",
    "        initial_count = len(df_final)\n",
    "\n",
    "        # Drop rows where both Report_Date and Sales are missing\n",
    "        df_final = df_final[~(df_final['Report_Date'].isna() & df_final['Sales'].isna())]\n",
    "        final_count = len(df_final)\n",
    "\n",
    "        # Convert Report_Date to string before writing to Excel\n",
    "        df_final['Report_Date'] = df_final['Report_Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "        # Save cleaned file\n",
    "        df_final.to_excel(final_output_path, index=False)\n",
    "        print(f\"\\n🧹 Cleanup complete: Removed {initial_count - final_count} invalid rows.\")\n",
    "\n",
    "    else:\n",
    "        print(\"⚠️ 'Sales' column missing — cleanup skipped.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Cleanup error: {e}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68b937a-26ca-414a-8e1f-fb6c6d871921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff4c0d-4736-408f-b9d1-53101fa98144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
